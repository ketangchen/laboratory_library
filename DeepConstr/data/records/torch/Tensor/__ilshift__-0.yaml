args:
  dtype:
  - Tensor
  - Tensor
  is_pos:
  - true
  - false
  name:
  - self
  - other
  required:
  - true
  - true
name: torch.Tensor.__ilshift__
package: torch
pass_rate: 100
rules:
- - cot: divided
    length: 1
    target:
      choosen_dtype:
        other: tensor
        self: tensor
      msg: '"lshift_cpu" not implemented for ''Half'''
      package: torch
    txt: (dtype(self) != "half")
  - f1_score: 99.4475138121547
    overall_score: 61.72375690607735
    precision: 100.0
    recall: 98.9010989010989
- - cot: 'To prevent the error "negative dimensions are not allowed", we need to ensure
      that none of the dimensions in both `self` and `other` tensors are negative.
      We can formulate the constraint as follows:'
    length: 1
    target:
      choosen_dtype:
        other: tensor
        self: tensor
      msg: negative dimensions are not allowed
      package: torch
    txt: all(self.shape[i] >= 0 for i in range(self.dim())) and all(other.shape[i]
      >= 0 for i in range(other.dim()))
  - f1_score: 69.6594427244582
    overall_score: 46.8297213622291
    precision: 100.0
    recall: 53.44418052256532
- - cot: 'Left : self.dim, self.shape Op : ==, and Right : other.dim, other.shape'
    length: 1
    target:
      choosen_dtype:
        other: tensor
        self: tensor
      msg: output with shape [] doesn't match the broadcast shape [7, 7, 7, 7, 7,
        7, 7]
      package: torch
    txt: self.dim == other.dim and self.shape == other.shape
  - f1_score: 72.63922518159806
    overall_score: 48.31961259079903
    precision: 100.0
    recall: 57.03422053231939
- - cot: 'The error is triggered because the output shape [] doesn''t match the broadcast
      shape [5, 2, 1, 3, 1, 2, 1]. This suggests that there is a mismatch in the shape
      of the tensors at non-singleton dimensions.


      To prevent this error, we need to ensure that the trailing dimensions of the
      tensors are either 1 or the same. We can formulate the constraint as follows:'
    length: 1
    target:
      choosen_dtype:
        other: tensor
        self: tensor
      msg: output with shape [] doesn't match the broadcast shape [5, 2, 1, 3, 1,
        2, 1]
      package: torch
    txt: all(self.shape[i] == other.shape[i] or self.shape[i] == 1 or other.shape[i]
      == 1 for i in range(-1, -min(len(self.shape), len(other.shape))-1, -1))
  - f1_score: 63.48949919224555
    overall_score: 46.666666666666664
    precision: 87.33333333333333
    recall: 49.8730964467005
- - cot: The error is triggered because the result type Half cannot be cast to the
      desired output type Int. To prevent this error, the dtypes of the two input
      tensors should be consistent.
    length: 1
    target:
      choosen_dtype:
        other: tensor
        self: tensor
      msg: result type Half can't be cast to the desired output type Int
      package: torch
    txt: self.dtype == other.dtype
  - f1_score: 67.67143933685004
    overall_score: 45.83571966842502
    precision: 100.0
    recall: 51.13895216400911
- - cot: 'The error message suggests that the ''__ilshift__'' function is not implemented
      for the ''Float'' dtype. To prevent this error, we need to add a constraint
      to ensure that the dtype of both ''self'' and ''other'' tensors is one of the
      allowed types. So, the correct constraint would be:'
    length: 1
    target:
      choosen_dtype:
        other: tensor
        self: tensor
      msg: '"lshift_cpu" not implemented for ''Float'''
      package: torch
    txt: self.dtype in ["half", "int16", "int32", "int64", "uint8", "uint16", "uint32",
      "uint64"] and other.dtype in ["half", "int16", "int32", "int64", "uint8", "uint16",
      "uint32", "uint64"]
  - f1_score: 97.82608695652173
    overall_score: 60.91304347826087
    precision: 100.0
    recall: 95.74468085106382
- - cot: synthesized
    length: 4
    target:
      choosen_dtype:
        other: tensor
        self: tensor
      msg: The size of tensor a (7) must match the size of tensor b (2) at non-singleton
        dimension 6
      package: torch
    txt: (((self.dim() == other.dim() and self.shape[6] == other.shape[6]) and (self.shape[1]
      == other.shape[1])) and (self.shape[i] == 1)) or (self.shape[i] == 1)
  - f1_score: 68.44106463878327
    overall_score: 100
    precision: 100.0
    recall: 52.02312138728323
- - cot: default
    length: 2
    target:
      choosen_dtype:
        other: Tensor
        self: Tensor
      msg: negative dimensions are not allowed
      package: torch
    txt: all(i >= 0 for i in self.shape) and all(i >= 0 for i in other.shape)
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
- - cot: default
    length: 2
    target:
      choosen_dtype:
        other: Tensor
        self: Tensor
      msg: Too large tensor shape
      package: torch
    txt: self.rank <= 7 and other.rank <= 7
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
- - cot: default
    length: 2
    target:
      choosen_dtype:
        other: Tensor
        self: Tensor
      msg: negative dimensions are not allowed
      package: torch
    txt: all(i >= 0 for i in self.shape) and all(i >= 0 for i in other.shape)
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
- - cot: default
    length: 2
    target:
      choosen_dtype:
        other: Tensor
        self: Tensor
      msg: Too large tensor shape
      package: torch
    txt: self.rank <= 7 and other.rank <= 7
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
