args:
  dtype:
  - Tensor
  - int
  - List[int]
  is_pos:
  - true
  - false
  - false
  name:
  - self
  - k
  - dims
  required:
  - true
  - false
  - false
name: torch.rot90
package: torch
pass_rate: 49.0
rules:
- - cot: 'The error occurs because the total rotation dimensions expected is 2, but
      the provided dimensions `[]` has a length of 0. To prevent this error in the
      future, we need to ensure that the total rotation dimensions is always equal
      to 2 for the given tensor. The constraint for the `dims` parameter can be expressed
      as:'
    length: 1
    target:
      choosen_dtype:
        dims: list[int]
        k: int
        self: tensor
      msg: expected total rotation dims == 2, but got dims = 0
      package: torch
    txt: len(dims) == 2
  - f1_score: 97.43589743589743
    overall_score: 50.5
    precision: 95.0
    recall: 100.0
- - cot: 'The error occurs because the value of `dim` provided is out of the valid
      range. In this case, the value of `dim` is 7, which is greater than the maximum
      valid value of 1.


      To prevent this error in the future and generate constraints that do not trigger
      it, we need to ensure that the value of `dim` is always within the valid range
      for the given tensor. If we denote the shape of the tensor as `n`, the valid
      range for the dimension would be `[-n, n-1]`.


      Therefore, the constraint for the `dim` parameter can be expressed as:'
    length: 1
    target:
      choosen_dtype:
        dims: list[int]
        k: int
        self: tensor
      msg: Rotation dim1 out of range, dim1 = 9
      package: torch
    txt: all(d >= -len(self.shape) and d < len(self.shape) for d in dims)
  - f1_score: 55.303030303030305
    overall_score: 39.5
    precision: 73.0
    recall: 44.512195121951216
- - cot: 'The error occurs because the rotation dimensions provided are the same,
      but they should be different. Let''s see what the runtime values of ''dims''
      were. It appears that ''dims'' is [-3, 0]. To prevent this error in the future
      and generate constraints that do not trigger it, we need to ensure that the
      two rotation dimensions are different. Therefore, the constraint for the ''dims''
      parameter can be expressed as:'
    length: 1
    target:
      choosen_dtype:
        dims: list[int]
        k: int
        self: tensor
      msg: expected rotation dims to be different, but got dim0 = 0 and dim1 = 0
      package: torch
    txt: dims[0] != dims[1]
  - f1_score: 91.51670951156812
    overall_score: 47.5
    precision: 89.0
    recall: 94.17989417989418
- - cot: default
    length: 1
    target:
      choosen_dtype:
        dims: List[int]
        k: int
        self: Tensor
      msg: negative dimensions are not allowed
      package: torch
    txt: all(i >= 0 for i in self.shape)
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
- - cot: default
    length: 1
    target:
      choosen_dtype:
        dims: List[int]
        k: int
        self: Tensor
      msg: Too large tensor shape
      package: torch
    txt: self.rank <= 7
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
