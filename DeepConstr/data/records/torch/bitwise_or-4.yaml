args:
  dtype:
  - Tensor
  - number
  - Tensor
  is_pos:
  - true
  - false
  - false
  name:
  - self
  - other
  - out
  required:
  - true
  - true
  - true
name: torch.bitwise_or
package: torch
pass_rate: 100
rules:
- - cot: divided
    length: 1
    target:
      choosen_dtype:
        other: int
        out: tensor
        self: tensor
      msg: '"bitwise_or_cpu" not implemented for ''Float'''
      package: torch
    txt: dtype(self) != "float32"
  - f1_score: 89.28571428571429
    overall_score: 100
    precision: 100.0
    recall: 80.64516129032259
- - cot: 'Based on the given runtime information, the error "Trying to resize storage
      that is not resizable" occurs when trying to resize the ''out'' tensor. To prevent
      this error, the shape and rank of the ''out'' tensor should match the shape
      and rank of the operation result, which is the ''self'' tensor. Therefore, the
      constraint to prevent the error is:'
    length: 1
    target:
      choosen_dtype:
        other: int
        out: tensor
        self: tensor
      msg: Trying to resize storage that is not resizable
      package: torch
    txt: out.rank == self.rank and all(out.shape[i] == self.shape[i] for i in range(out.rank))
  - f1_score: 74.44168734491315
    overall_score: 49.220843672456574
    precision: 100.0
    recall: 59.28853754940712
- - cot: default
    length: 2
    target:
      choosen_dtype:
        other: number
        out: Tensor
        self: Tensor
      msg: negative dimensions are not allowed
      package: torch
    txt: all(i >= 0 for i in self.shape) and all(i >= 0 for i in out.shape)
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
- - cot: default
    length: 2
    target:
      choosen_dtype:
        other: number
        out: Tensor
        self: Tensor
      msg: Too large tensor shape
      package: torch
    txt: self.rank <= 7 and out.rank <= 7
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
