args:
  dtype:
  - tensor
  - tensor
  - str
  is_pos:
  - false
  - false
  - false
  name:
  - x
  - y
  - name
  required:
  - true
  - true
  - false
name: tf.math.multiply_no_nan
package: null
pass_rate: 100
rules:
- - cot: divided
    length: 1
    target:
      choosen_dtype:
        name: str
        x: tensor
        y: tensor
      msg: "Value for attr 'T' of int32 is not in the list of allowed values: bfloat16,\
        \ half, float, double, complex64, complex128\n\t; NodeDef: {{node MulNoNan}};\
        \ Op<name=MulNoNan; signature=x:T, y:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16,\
        \ DT_HALF, DT_FLOAT, DT_DOUBLE, DT_COMPLEX64, DT_COMPLEX128]> [Op:MulNoNan]"
      package: tensorflow
    txt: dtype(y) in ['bfloat16', 'half', 'float', 'double', 'complex64', 'complex128']
  - f1_score: 100.0
    overall_score: 100
    precision: 100.0
    recall: 100.0
- - cot: 'The error is triggered because there is a type conversion issue. The function
      tf.math.multiply_no_nan expects the input tensors x and y to have the same data
      type, but in this case, x has a data type of int32 and y has a data type of
      float32.


      To prevent this error, we need to ensure that the data types of x and y are
      the same. We can add a constraint to check if the data types are equal:'
    length: 1
    target:
      choosen_dtype:
        name: str
        x: tensor
        y: tensor
      msg: "y: Tensor conversion requested dtype float32 for Tensor with dtype int32:\
        \ <tf.Tensor: shape=(1, 1, 1, 7, 1, 7, 7), dtype=int32, numpy=\narray([[[[[[[-804233,\
        \  393725,  -97352, -891546,  981842,  546099,\n              430856],\n \
        \           [-876424,  761446, -999860, -364985, -411695,  579808,\n     \
        \        -760891],\n            [ 143342, -722552, -424266,  858463, -272203,\
        \  990917,\n              690786],\n            [-180814,   73600,  149325,\
        \  209468,  513724,   45975,\n             -896464],\n            [ 640331,\
        \  742606,  332684, -234032,  581932, -914156,\n             -393572],\n \
        \           [ 359847,  561472,  274052,  585981,  396858,  272219,\n     \
        \        -861090],\n            [-444554, -122765, -853747,   19743,  463001,\
        \  929718,\n              544135]]],\n\n\n          [[[ 684866,  423829, \
        \ 169263, -731094, -997941, -821465,\n             -741989],\n           \
        \ [ 316317,  243134, -737084,  154604,  829856,   96060,\n             -659428],\n\
        \            [-619910, -879920, -529143,  -98153,  547483,  162335,\n    \
        \         -748622],\n            [-423315, -489322, -512178, -474365, -393576,\
        \ -425743,\n              765507],\n            [ 383607,  581944, -127108,\
        \  290144,  759729,  757455,\n             -325770],\n            [ 230563,\
        \ -862830, -397328, -102385,  -81547, -584486,\n              976433],\n \
        \           [-660831, -298127,  369689,  619137,  114713,  521701,\n     \
        \        -660411]]],\n\n\n          [[[ 567650,  378975, -759034,  873635,\
        \ -164389, -503531,\n             -178438],\n            [ 996885,  828503,\
        \  519507,  654588, -812434, -158265,\n             -980487],\n          \
        \  [ 960143,  397766,  275908,  305116,  -92633,  520008,\n              \
        \ 90495],\n            [ 524225,  489004,   60895, -162272,   71546,  887985,\n\
        \              633566],\n            [ 776740,  297848, -146709,  103439,\
        \  598812,  507995,\n              220123],\n            [ 193167,   85916,\
        \  455797, -419342, -140881,  467294,\n             -302640],\n          \
        \  [-478633,  816039,  257618, -298873,  583298, -787150,\n              244877]]],\n\
        \n\n          [[[ 808962, -726278,  942507,  730949,  783083,  678299,\n \
        \             447865],\n            [-483318, -553787, -245061, -483859, \
        \ 900115, -827274,\n             -329041],\n            [ 973086,  486452,\
        \  400071, -381242,  346152,  651653,\n              658719],\n          \
        \  [ 731614,  617385,  987849,  996660,  989477, -291844,\n              -37780],\n\
        \            [ 372940, -207220,  -96890, -657703, -310973, -837370,\n    \
        \         -727305],\n            [ 998460,  544261, -525884,  404345, -389050,\
        \  591205,\n              -92832],\n            [ 664410, -399081,  961315,\
        \  649504, -586739,  -88511,\n             -195480]]],\n\n\n          [[[-625497,\
        \  330122, -156847,  714204,  429333, -493108,\n             -825871],\n \
        \           [-471514,  971246,  843238,  613872, -600115, -529859,\n     \
        \         335256],\n            [ 158013,  -53259,  955205,  210585,  172559,\
        \  238458,\n              305328],\n            [ 447411,  866802, -508900,\
        \ -181481, -845745, -363792,\n             -835705],\n            [ 570519,\
        \ -297186,  364605,  373675,  531556, -467232,\n             -255871],\n \
        \           [-121254, -661577,  545509,  171330, -552726,  328243,\n     \
        \        -927068],\n            [-458860,  826371,  140668,  266698, -974969,\
        \ -371492,\n              952024]]],\n\n\n          [[[-186839, -160346, \
        \ 169719,  631610,  475477,  308598,\n             -624630],\n           \
        \ [-832509,  -31961,  295785,  673912, -224442,  204937,\n              323384],\n\
        \            [-597938,  195911,  120657,  575313,  725945,  432662,\n    \
        \         -324751],\n            [-315588, -536176, -364486, -389279,  260655,\
        \   72583,\n              895774],\n            [-266831, -198444,   50529,\
        \ -529101,  586407,  529983,\n             -188487],\n            [ 450300,\
        \  775056,  540738, -155932,  985041, -106849,\n              282865],\n \
        \           [ 759800,  199961, -405724,  262982, -975449,   42506,\n     \
        \         815729]]],\n\n\n          [[[ 841857,  803367,  493198,  834728,\
        \  806822, -834661,\n              159988],\n            [-971735,  545643,\
        \  961854, -267640, -738465,  874089,\n             -972201],\n          \
        \  [-924509, -169322, -103200,  337868, -123844,  358876,\n              161955],\n\
        \            [ 675142, -410509, -616458, -822934, -643981,  315244,\n    \
        \         -172794],\n            [ 230835,  623639,  624909, -231343,  546333,\
        \  923624,\n              985107],\n            [ 205698, -761532,  912901,\
        \ -441887, -368216, -963800,\n              941964],\n            [-713142,\
        \ -811011,  453802, -540266,  653483, -151398,\n             -477098]]]]]]],\
        \ dtype=int32)>"
      package: tensorflow
    txt: x.dtype == y.dtype
  - f1_score: 100.0
    overall_score: 100
    precision: 100.0
    recall: 100.0
- - cot: synthesized
    length: 3
    target:
      choosen_dtype:
        name: str
        x: tensor
        y: tensor
      msg: '{{function_node __wrapped__MulNoNan_device_/job:localhost/replica:0/task:0/device:CPU:0}}
        Incompatible shapes: [5,5,5,5,5,5] vs. [1,4,1,1,3,1,1] [Op:MulNoNan]'
      package: tensorflow
    txt: ((all(x.shape[i] == y.shape[i] or x.shape[i] == 1 or y.shape[i] == 1 for
      i in range(-1, -min(len(x.shape), len(y.shape))-1, -1))) and (len(x) == len(y)))
      and (x.shape == y.shape)
  - f1_score: 96.32918417505748
    overall_score: 60.16459208752874
    precision: 96.42857142857143
    recall: 96.23000158403295
- - cot: default
    length: 2
    target:
      choosen_dtype:
        name: str
        x: tensor
        y: tensor
      msg: negative dimensions are not allowed
      package: null
    txt: all(i >= 0 for i in x.shape) and all(i >= 0 for i in y.shape)
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
- - cot: default
    length: 2
    target:
      choosen_dtype:
        name: str
        x: tensor
        y: tensor
      msg: Too large tensor shape
      package: null
    txt: x.rank <= 7 and y.rank <= 7
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
