args:
  dtype:
  - tensor
  - tensor
  - str
  is_pos:
  - false
  - false
  - false
  name:
  - a
  - x
  - name
  required:
  - true
  - true
  - false
name: tf.math.igammac
package: null
pass_rate: 100
rules:
- - cot: synthesized
    length: 3
    target:
      choosen_dtype:
        a: tensor
        name: str
        x: tensor
      msg: '{{function_node __wrapped__Igammac_device_/job:localhost/replica:0/task:0/device:CPU:0}}
        Incompatible shapes: [3,6,5,1,1,3,1] vs. [2,7,2,7,3,2,7] [Op:Igammac] name:
        gtTI'
      package: tensorflow
    txt: ((a.shape == x.shape) and (all(a.shape[i] == x.shape[i] or a.shape[i] ==
      1 or x.shape[i] == 1 for i in range(-1, -min(len(a.shape), len(x.shape))-1,
      -1)))) and (len(a) == len(x))
  - f1_score: 94.9367088607595
    overall_score: 100
    precision: 100.0
    recall: 90.36144578313254
- - cot: divided
    length: 1
    target:
      choosen_dtype:
        a: tensor
        name: str
        x: tensor
      msg: "Value for attr 'T' of int32 is not in the list of allowed values: bfloat16,\
        \ half, float, double\n\t; NodeDef: {{node Igammac}}; Op<name=Igammac; signature=a:T,\
        \ x:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>\
        \ [Op:Igammac] name: GZMk"
      package: tensorflow
    txt: dtype(a) in ['bfloat16', 'half', 'float', 'double']
  - f1_score: 100.0
    overall_score: 100
    precision: 100.0
    recall: 100.0
- - cot: The error is due to the use of an unsupported data type, DT_BFLOAT16, on
      the Igammac function. The function tf.math.igammac does not support this data
      type. Therefore, the dtype of the tensors 'a' and 'x' should be corrected. The
      valid data types include float32 and float64. So, dtype(a) and dtype(x) should
      not be equal to DT_BFLOAT16.
    length: 1
    target:
      choosen_dtype:
        a: tensor
        name: str
        x: tensor
      msg: "Could not find device for node: {{node Igammac}} = Igammac[T=DT_BFLOAT16]\n\
        All kernels registered for op Igammac:\n  device='XLA_CPU_JIT'; T in [DT_FLOAT,\
        \ DT_DOUBLE, DT_BFLOAT16, DT_HALF]\n  device='XLA_GPU_JIT'; T in [DT_FLOAT,\
        \ DT_DOUBLE, DT_BFLOAT16, DT_HALF]\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU';\
        \ T in [DT_FLOAT]\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU'; T in\
        \ [DT_FLOAT]\n [Op:Igammac] name: YInz"
      package: tensorflow
    txt: (a.dtype != 'DT_BFLOAT16') and (x.dtype != 'DT_BFLOAT16')
  - f1_score: 97.39696312364426
    overall_score: 100
    precision: 100.0
    recall: 94.92600422832982
- - cot: synthesized
    length: 4
    target:
      choosen_dtype:
        a: tensor
        name: str
        x: tensor
      msg: 'cannot compute Igammac as input #1(zero-based) was expected to be a float
        tensor but is a int32 tensor [Op:Igammac] name: CVUK'
      package: tensorflow
    txt: (((dtype(x) == tf.float32) or (dtype(a) == tf.float64)) and (dtype(x) ==
      float)) and (dtype(x) == 'double')
  - f1_score: 69.34362934362936
    overall_score: 100
    precision: 100.0
    recall: 53.07328605200946
- - cot: default
    length: 2
    target:
      choosen_dtype:
        a: tensor
        name: str
        x: tensor
      msg: negative dimensions are not allowed
      package: null
    txt: all(i >= 0 for i in a.shape) and all(i >= 0 for i in x.shape)
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
- - cot: default
    length: 2
    target:
      choosen_dtype:
        a: tensor
        name: str
        x: tensor
      msg: Too large tensor shape
      package: null
    txt: a.rank <= 7 and x.rank <= 7
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
