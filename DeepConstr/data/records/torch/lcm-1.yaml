args:
  dtype:
  - Tensor
  - Tensor
  - Tensor
  is_pos:
  - true
  - false
  - false
  name:
  - self
  - other
  - out
  required:
  - true
  - true
  - true
name: torch.lcm
package: torch
pass_rate: 100
rules:
- - cot: 'Based on the given runtime information, the error occurs because the size
      of the tensor "a" is 9, while the size of the tensor "b" is 3 at non-singleton
      dimension 3. To prevent this error, the following constraint can be formulated:'
    length: 1
    target:
      choosen_dtype:
        other: tensor
        out: tensor
        self: tensor
      msg: The size of tensor a (8) must match the size of tensor b (6) at non-singleton
        dimension 2
      package: torch
    txt: self.shape[3] == other.shape[3]
  - f1_score: 74.07407407407409
    overall_score: 100
    precision: 100.0
    recall: 58.82352941176471
- - cot: synthesized
    length: 2
    target:
      choosen_dtype:
        other: tensor
        out: tensor
        self: tensor
      msg: Trying to resize storage that is not resizable
      package: torch
    txt: (out.rank == self.rank) and (out.shape == self.shape)
  - f1_score: 61.84971098265896
    overall_score: 28.25
    precision: 53.5
    recall: 73.28767123287672
- - cot: "The error occurs because the function torch.lcm does not have an implementation\
      \ for the data type 'Float'. To prevent the error, the data type of the input\
      \ tensors 'other' and 'self' should be one of the allowed types, which are 'half',\
      \ 'int16', 'int32', 'int64', 'uint8', 'uint16', 'uint32', 'uint64'. \n\nWrap\
      \ the final formula with"
    length: 1
    target:
      choosen_dtype:
        other: tensor
        out: tensor
        self: tensor
      msg: '"lcm_cpu" not implemented for ''Float'''
      package: torch
    txt: dtype(other) in ["half", "int16", "int32", "int64", "uint8", "uint16", "uint32",
      "uint64"] and dtype(self) in ["half", "int16", "int32", "int64", "uint8", "uint16",
      "uint32", "uint64"]
  - f1_score: 78.27788649706457
    overall_score: 100
    precision: 100.0
    recall: 64.30868167202571
- - cot: 'The error arises because the function torch.lcm is not implemented for the
      data type ''Half''. The input tensor ''self'' is of type ''Half'' which is not
      compatible with the function. Therefore, the data type of ''self'' should be
      changed to one of the allowed types. Op : in, and Right : [float16, float32,
      int8, int16, int32, int64, uint8, uint16, uint32, uint64].'
    length: 1
    target:
      choosen_dtype:
        other: tensor
        out: tensor
        self: tensor
      msg: '"lcm_cpu" not implemented for ''Half'''
      package: torch
    txt: dtype(self) in [float32, int8, int16, int32, int64, uint8, uint16, uint32,
      uint64]
  - f1_score: 84.92569002123143
    overall_score: 100
    precision: 100.0
    recall: 73.80073800738008
- - cot: synthesized
    length: 3
    target:
      choosen_dtype:
        other: tensor
        out: tensor
        self: tensor
      msg: '"lcm_cpu" not implemented for ''Half'''
      package: torch
    txt: ((dtype(self) in ["int8", "int16", "int32", "int64", "uint8", "uint16", "uint32",
      "uint64"] and dtype(other) in ["int8", "int16", "int32", "int64", "uint8", "uint16",
      "uint32", "uint64"]) or (dtype(out) != "Half")) or (dtype(other) in ["int8",
      "int16", "int32", "int64", "uint8", "uint16", "uint32", "uint64"])
  - f1_score: 100.0
    overall_score: 100
    precision: 100.0
    recall: 100.0
- - cot: The error is due to an attempt to resize a tensor that is not resizable.
      In this case, the 'out' tensor's size should match the size of the operation's
      result. The operation result's shape is determined by the size of the 'self'
      and 'other' tensors. Therefore, the shape and rank of 'out' should be equal
      to the shape and rank of 'other' and 'self'.
    length: 1
    target:
      choosen_dtype:
        other: tensor
        out: tensor
        self: tensor
      msg: Trying to resize storage that is not resizable
      package: torch
    txt: out.rank==other.rank and all(out.shape[i]==other.shape[i] for i in range(out.rank))
  - f1_score: 66.77796327212019
    overall_score: 45.388981636060095
    precision: 100.0
    recall: 50.125313283208015
- - cot: default
    length: 3
    target:
      choosen_dtype:
        other: Tensor
        out: Tensor
        self: Tensor
      msg: negative dimensions are not allowed
      package: torch
    txt: all(i >= 0 for i in self.shape) and all(i >= 0 for i in other.shape) and
      all(i >= 0 for i in out.shape)
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
- - cot: default
    length: 3
    target:
      choosen_dtype:
        other: Tensor
        out: Tensor
        self: Tensor
      msg: Too large tensor shape
      package: torch
    txt: self.rank <= 7 and other.rank <= 7 and out.rank <= 7
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
