args:
  dtype:
  - number
  - Tensor
  - Tensor
  is_pos:
  - false
  - false
  - false
  name:
  - x
  - n
  - out
  required:
  - true
  - true
  - true
name: torch._C._special.special_hermite_polynomial_h
package: torch
pass_rate: 91.0
rules:
- - cot: Based on the given runtime information, the error occurs because the result
      type of `special_hermite_polynomial_h` function is Float, but it cannot be cast
      to the desired output type Int. To prevent this error, the output tensor `out`
      should have a data type of Float instead of Int.
    length: 1
    target:
      choosen_dtype:
        n: tensor
        out: tensor
        x: int
      msg: result type Float can't be cast to the desired output type Int
      package: torch
    txt: dtype(out) == Float
  - f1_score: 100.0
    overall_score: 100
    precision: 100.0
    recall: 100.0
- - cot: '`out` tensor should have the same number of dimensions as the `n` tensor,
      and the shape of each dimension in `out` should be equal to or greater than
      the corresponding dimension in `n` tensor.'
    length: 1
    target:
      choosen_dtype:
        n: tensor
        out: tensor
        x: int
      msg: Trying to resize storage that is not resizable
      package: torch
    txt: out.ndim == n.ndim and all(out.shape[i] >= n.shape[i] for i in range(n.ndim))
  - f1_score: 86.58008658008657
    overall_score: 55.290043290043286
    precision: 100.0
    recall: 76.33587786259541
- - cot: default
    length: 2
    target:
      choosen_dtype:
        n: Tensor
        out: Tensor
        x: number
      msg: negative dimensions are not allowed
      package: torch
    txt: all(i >= 0 for i in n.shape) and all(i >= 0 for i in out.shape)
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
- - cot: default
    length: 2
    target:
      choosen_dtype:
        n: Tensor
        out: Tensor
        x: number
      msg: Too large tensor shape
      package: torch
    txt: n.rank <= 7 and out.rank <= 7
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
