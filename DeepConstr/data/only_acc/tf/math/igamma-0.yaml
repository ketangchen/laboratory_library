args:
  dtype:
  - tensor
  - tensor
  - str
  is_pos:
  - false
  - false
  - false
  name:
  - a
  - x
  - name
  required:
  - true
  - true
  - false
name: tf.math.igamma
package: null
pass_rate: 100
rules:
- - cot: synthesized
    length: 3
    target:
      choosen_dtype:
        a: tensor
        name: str
        x: tensor
      msg: '{{function_node __wrapped__Igamma_device_/job:localhost/replica:0/task:0/device:CPU:0}}
        Incompatible shapes: [6,7,6,5,3,1,3] vs. [2,2,0,0,1,4,2] [Op:Igamma] name:
        qCAZ'
      package: tensorflow
    txt: ((all(a.shape[i] == x.shape[i] or a.shape[i] == 1 or x.shape[i] == 1 for
      i in range(-1, -min(len(a.shape), len(x.shape))-1, -1))) or (a.shape == x.shape))
      and (len(a.shape) == len(x.shape))
  - f1_score: 91.18541033434651
    overall_score: 100
    precision: 100.0
    recall: 83.79888268156425
- - cot: divided
    length: 1
    target:
      choosen_dtype:
        a: tensor
        name: str
        x: tensor
      msg: "Value for attr 'T' of int32 is not in the list of allowed values: bfloat16,\
        \ half, float, double\n\t; NodeDef: {{node Igamma}}; Op<name=Igamma; signature=a:T,\
        \ x:T -> z:T; attr=T:type,allowed=[DT_BFLOAT16, DT_HALF, DT_FLOAT, DT_DOUBLE]>\
        \ [Op:Igamma] name: JCCG"
      package: tensorflow
    txt: dtype(a) in ['bfloat16', 'half', 'float', 'double']
  - f1_score: 100.0
    overall_score: 100
    precision: 100.0
    recall: 100.0
- - cot: 'The error indicates that the function tf.math.igamma is trying to run on
      a data type (DT_BFLOAT16) that is not supported by the device. This could be
      because the device does not support BFLOAT16 operations or the TensorFlow version
      used does not support this operation on the device. Therefore, the datatype
      of ''a'' and ''x'' should not be ''DT_BFLOAT16''. So, Left : a.dtype, x.dtype,
      Op : !=, Right : ''DT_BFLOAT16''.'
    length: 1
    target:
      choosen_dtype:
        a: tensor
        name: str
        x: tensor
      msg: "Could not find device for node: {{node Igamma}} = Igamma[T=DT_BFLOAT16]\n\
        All kernels registered for op Igamma:\n  device='XLA_CPU_JIT'; T in [DT_FLOAT,\
        \ DT_DOUBLE, DT_BFLOAT16, DT_HALF]\n  device='XLA_GPU_JIT'; T in [DT_FLOAT,\
        \ DT_DOUBLE, DT_BFLOAT16, DT_HALF]\n  device='CPU'; T in [DT_DOUBLE]\n  device='CPU';\
        \ T in [DT_FLOAT]\n  device='GPU'; T in [DT_DOUBLE]\n  device='GPU'; T in\
        \ [DT_FLOAT]\n [Op:Igamma] name: Ztyu"
      package: tensorflow
    txt: a.dtype != 'DT_BFLOAT16'
  - f1_score: 98.4771573604061
    overall_score: 51.5
    precision: 97.0
    recall: 100.0
- - cot: synthesized
    length: 2
    target:
      choosen_dtype:
        a: tensor
        name: str
        x: tensor
      msg: 'cannot compute Igamma as input #1(zero-based) was expected to be a double
        tensor but is a float tensor [Op:Igamma] name: MMXA'
      package: tensorflow
    txt: (dtype(x) == tf.float64) and (dtype(a) == tf.float64)
  - f1_score: 78.53403141361257
    overall_score: 100
    precision: 100.0
    recall: 64.65517241379311
- - cot: default
    length: 2
    target:
      choosen_dtype:
        a: tensor
        name: str
        x: tensor
      msg: negative dimensions are not allowed
      package: null
    txt: all(i >= 0 for i in a.shape) and all(i >= 0 for i in x.shape)
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
- - cot: default
    length: 2
    target:
      choosen_dtype:
        a: tensor
        name: str
        x: tensor
      msg: Too large tensor shape
      package: null
    txt: a.rank <= 7 and x.rank <= 7
  - f1_score: -1
    overall_score: -1
    precision: -1
    recall: -1
